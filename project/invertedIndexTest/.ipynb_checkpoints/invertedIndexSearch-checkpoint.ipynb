{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd22e79e-f08c-4f3d-b65b-ae1f2dc53adb",
   "metadata": {},
   "source": [
    "# Inverted Index Search from Indexed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47063e8c-f794-4a8f-8c93-cfb4d07a01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Read the csv file\n",
    "# invertedIndex file\n",
    "# OneDrive/Documents/GitHub/swdev02_work/project/temp_src/invertedIndex_v0.csv\n",
    "invertedIndex = pd.read_csv('invertedIndexTest/csv/invertedIndex_v1.csv', index_col=0)\n",
    "# webData file\n",
    "webData = pd.read_csv('invertedIndexTest/csv/webData_v0.csv', index_col=0)\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def normalize(self, raw_text):\n",
    "        \"\"\"Remove special characters and lowercase text\"\"\"\n",
    "        return re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", raw_text.lower())\n",
    "\n",
    "    def remove_stopwords(self, raw_text):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        words = [word for word in raw_text.split() if word not in self.stop_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def lemmatize(self, raw_text):\n",
    "        \"\"\"Perform lemmatization\"\"\"\n",
    "        doc = self.nlp(raw_text)\n",
    "        return [token.lemma_ for token in doc]\n",
    "\n",
    "    def clean(self, raw_text):\n",
    "        \"\"\"Clean text by normalizing, removing stopwords, and lemmatizing\"\"\"\n",
    "        raw_text = self.normalize(raw_text)\n",
    "        raw_text = self.remove_stopwords(raw_text)\n",
    "        return self.lemmatize(raw_text)\n",
    "\n",
    "\n",
    "def get_common_id(lists):\n",
    "    # Initialize an empty set to store the common elements\n",
    "    common_data = set(lists[0])\n",
    "    # Iterate over the rest of the lists\n",
    "    for lst in lists[1:]:\n",
    "        # Update the set with the common elements of the current list and the set\n",
    "        common_data.intersection_update(set(lst))\n",
    "    return list(common_data)\n",
    "\n",
    "def return_url_by_id(id):    \n",
    "    try:\n",
    "        return webData.loc[webData['ID'] == id, 'URL'][0]\n",
    "    except KeyError:\n",
    "        return 'No ID found in webData'\n",
    "\n",
    "# Input Query convert to Token\n",
    "def query_clean(input_str):\n",
    "    if (input_str != \"\") and (type(input_str) == str):\n",
    "        text_cleaner = TextCleaner()\n",
    "        normalized_text = text_cleaner.normalize(input_str)\n",
    "        no_stopwords_text = text_cleaner.remove_stopwords(normalized_text)\n",
    "        lemmatized_text = text_cleaner.lemmatize(no_stopwords_text)\n",
    "        return lemmatized_text\n",
    "    else:\n",
    "        return 'Input Error'\n",
    "\n",
    "# Token to ID list\n",
    "def token_to_match_list_list(token_list):\n",
    "    res_temp = []\n",
    "    for token in token_list:\n",
    "        # get dict of each token and make a list\n",
    "        res_temp.append( ast.literal_eval(invertedIndex.loc[invertedIndex['Gram'] == token, 'DocsID_Dict'][0]) )\n",
    "    return res_temp\n",
    "\n",
    "def search_inverted_index(inputQuery):\n",
    "    output_temp = get_common_id(token_to_match_list_list(query_clean(inputQuery)))\n",
    "    print('Total ', len(output_temp), \" Results\")\n",
    "    for id in output_temp:\n",
    "        print(return_url_by_id(id))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0444760f-5dae-4d22-9a6a-fe8cbe99193e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Search : \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "None !ex\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching :  !ex\n",
      "\n",
      "EXIT . . .\n"
     ]
    }
   ],
   "source": [
    "inputQuery = \"\"\n",
    "\n",
    "while inputQuery != \"!ex\":\n",
    "    inputQuery = input(print(\"Input Search : \"))\n",
    "    print(\"Searching : \", inputQuery)\n",
    "    if inputQuery == \"!ex\":\n",
    "        pass\n",
    "    else:\n",
    "        search_inverted_index(inputQuery)    \n",
    "    \n",
    "print(\"\\nEXIT . . .\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8759e-422b-49cb-804c-809d4b17cb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "29779eaeaa662f518a7c5365664c9c0d0a52b94abeecfec7b0ccc4fc94e18524"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
