{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3a34e5-b45e-4b2c-a003-84f39a1344c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import mysql.connector\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0423c8-c3dd-4ab8-87d4-b7cb1af3c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.35mmc.com/26/09/2016/leica-c2-zoom-review/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e870a8a2-9b61-48af-8344-96035c8259f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "webReq = requests.get(url)\n",
    "soup_obj = BeautifulSoup(webReq.text, \"html.parser\")\n",
    "if soup_obj.find('article').text != None:\n",
    "    raw_text = soup_obj.find('article').text\n",
    "    # get title\n",
    "    get_title = soup_obj.find('h1', {'class':'entry-title'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "293c41c6-54d6-4699-bfa0-c1b07bc35225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leica C2 Zoom Review – A Mind of Its Own – by Torsten Kathke'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1dbc8-5e04-48b5-8637-0016bd270fd6",
   "metadata": {},
   "source": [
    "# Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b79ef9f-60fd-4e2b-ba60-ea3d89218455",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_norm = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8b251-e5f0-462b-90ce-7ac544f856ed",
   "metadata": {},
   "source": [
    "# Clear Unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597bd982-2b95-4776-b09b-892b9cd044dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "raw_text_norm_uni = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", raw_text_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132029c-1341-4dd6-96a0-df675ae2de8f",
   "metadata": {},
   "source": [
    "# Remove stopwords + tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af73e58b-8771-4fce-804a-ed75dc1b4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with nltk\n",
    "import nltk.corpus as nltkc\n",
    "\n",
    "stopwords = set(nltkc.stopwords.words('english'))\n",
    "# tokenized\n",
    "raw_text_norm_uni_stopnltk_token = []\n",
    "\n",
    "for word in raw_text_norm_uni.split():\n",
    "    if word not in stopwords:\n",
    "        raw_text_norm_uni_stopnltk_token.append(word)\n",
    "\n",
    "# joined\n",
    "raw_text_norm_uni_stopnltk_join = \" \".join(raw_text_norm_uni_stopnltk_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a75ced-c0ca-4fb0-b64f-b98f1e4be92a",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32452508-26d3-44e0-a2ed-0b0a1c7643d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # import these modules\n",
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# ps = PorterStemmer()\n",
    "\n",
    "# for w in raw_text_norm_uni_stopnltk_token:\n",
    "#     print(w, \" : \", ps.stem(w))\n",
    "\n",
    "import nltk\n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# for token in raw_text_norm_uni_stopnltk_token:\n",
    "#     print(token, \" : \", lemma.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "37b39c04-67c7-4cce-970e-ae265c19d434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# from nltk.stem import PorterStemmer\n",
    "\n",
    "clean_token = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(raw_text_norm_uni_stopnltk_join)\n",
    "for token in doc:\n",
    "    # print(token, \" : \", token.lemma_)\n",
    "    clean_token.append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a1ad0-55f4-45fd-83c2-75eb5e347af2",
   "metadata": {},
   "source": [
    "# Full Function\n",
    "#### Normalize + Unicode Removed + Stopword Removed(NLTK) + Stemming(SPACY) + Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0a0d89-3b10-47d4-b61e-e0063d35810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk.corpus as nltkc\n",
    "import spacy\n",
    "# ------------------------------------------------\n",
    "# ------------------------------------------------\n",
    "def txt_cleaner(raw_text):\n",
    "    raw_text_norm = raw_text.lower()\n",
    "    raw_text_norm_uni = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", raw_text_norm)\n",
    "    # ------------------------------------------------\n",
    "    stopwords = set(nltkc.stopwords.words('english'))\n",
    "    raw_text_norm_uni_stopnltk_token = []\n",
    "    for word in raw_text_norm_uni.split():\n",
    "        if word not in stopwords:\n",
    "            raw_text_norm_uni_stopnltk_token.append(word)\n",
    "    raw_text_norm_uni_stopnltk_join = \" \".join(raw_text_norm_uni_stopnltk_token)\n",
    "    # ------------------------------------------------\n",
    "    clean_token = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(raw_text_norm_uni_stopnltk_join)\n",
    "    for token in doc:\n",
    "        # print(token, \" : \", token.lemma_)\n",
    "        clean_token.append(token)\n",
    "    return clean_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc743c8e-5fe1-4a9a-b725-6e7b9d21b348",
   "metadata": {},
   "source": [
    "# finalfunction : raw text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6414dad-dbae-4f74-96a8-7a03251f02d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def txt_cleaner(raw_text):\n",
    "    \"\"\"Normalize + Unicode Removed + Stopword Removed(NLTK) + Stemming(SPACY) + Tokenized\"\"\"\n",
    "    \n",
    "    # Normalize and remove special characters\n",
    "    raw_text = re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", raw_text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in raw_text.split() if word not in stop_words]\n",
    "    raw_text = \" \".join(words)\n",
    "    \n",
    "    # Perform lemmatization\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(raw_text)\n",
    "    clean_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8892f05-6185-4f91-9fe1-e399bac9d845",
   "metadata": {},
   "source": [
    "# Final Function in OOP Class form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116fc91f-8917-46d5-8eea-c9c271f0b97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def normalize(self, raw_text):\n",
    "        \"\"\"Remove special characters and lowercase text\"\"\"\n",
    "        return re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", raw_text.lower())\n",
    "\n",
    "    def remove_stopwords(self, raw_text):\n",
    "        \"\"\"Remove stopwords\"\"\"\n",
    "        words = [word for word in raw_text.split() if word not in self.stop_words]\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def lemmatize(self, raw_text):\n",
    "        \"\"\"Perform lemmatization\"\"\"\n",
    "        doc = self.nlp(raw_text)\n",
    "        return [token.lemma_ for token in doc]\n",
    "\n",
    "    def clean(self, raw_text):\n",
    "        \"\"\"Clean text by normalizing, removing stopwords, and lemmatizing\"\"\"\n",
    "        raw_text = self.normalize(raw_text)\n",
    "        raw_text = self.remove_stopwords(raw_text)\n",
    "        return self.lemmatize(raw_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e636ee67-73af-49f3-8e5a-4c12ea16b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner = TextCleaner()\n",
    "normalized_text = text_cleaner.normalize(raw_text)\n",
    "no_stopwords_text = text_cleaner.remove_stopwords(normalized_text)\n",
    "lemmatized_text = text_cleaner.lemmatize(no_stopwords_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800d3ae2-7be8-4460-97ee-6a8de5435f3c",
   "metadata": {},
   "source": [
    "# Unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05f1ec53-d404-472e-bf19-7e5da12b3024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_clean (__main__.TestTextCleaner) ... ok\n",
      "test_normalize (__main__.TestTextCleaner) ... ok\n",
      "test_remove_stopwords (__main__.TestTextCleaner) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 1.954s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1d657b08fd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestTextCleaner(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.text_cleaner = TextCleaner()\n",
    "        self.raw_text = \"This is some raw text that needs to be cleaned.\"\n",
    "        self.normalized_text = \"this is some raw text that needs to be cleaned \"\n",
    "        self.normalized_text_false = \"this is some raw text that needs to be cleaned\"\n",
    "        self.no_stopwords_text = \"This raw text needs cleaned.\"\n",
    "        self.no_stopwords_text_false = \"This raw text needs cleaned\"\n",
    "        self.lemmatized_text = ['raw', 'text', 'need', 'clean']\n",
    "\n",
    "    def test_normalize(self):\n",
    "        self.assertEqual(self.text_cleaner.normalize(self.raw_text), self.normalized_text)\n",
    "        self.assertNotEqual(self.text_cleaner.normalize(self.raw_text), self.normalized_text_false)\n",
    "\n",
    "    def test_remove_stopwords(self):\n",
    "        self.assertEqual(self.text_cleaner.remove_stopwords(self.raw_text), self.no_stopwords_text)\n",
    "        self.assertNotEqual(self.text_cleaner.remove_stopwords(self.raw_text), self.raw_text)\n",
    "\n",
    "#     Cannot test due to depends on each library's algorithm\n",
    "#     def test_lemmatize(self):\n",
    "#         self.assertEqual(self.text_cleaner.lemmatize(self.no_stopwords_text), self.lemmatized_text)\n",
    "        \n",
    "    def test_clean(self):\n",
    "        self.assertEqual(self.text_cleaner.clean(self.raw_text), self.lemmatized_text)\n",
    "        self.assertNotEqual(self.text_cleaner.clean(self.raw_text), self.raw_text)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     unittest.main()\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3670ad2-30fd-46e0-b688-ba16e770ca14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw', 'text', 'need', 'clean']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner = TextCleaner()\n",
    "normalized_text = text_cleaner.clean('This is some raw text that needs to be cleaned.')\n",
    "normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196e041-47c2-420a-bff3-e7b047d68674",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
